{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR Training Workshop for Forest Degradation\n",
    "\n",
    "# PART 5 - Cumulative Sum Classifier\n",
    "\n",
    "Josef Kellndorfer, Ph.D., President and Senior Scientist, Earth Big Data, LLC\n",
    "\n",
    "Revision date: June 2020 Virtual Training\n",
    "\n",
    "In this chapter we are looking at the test site in Peru. We will study segmentation of the time series into a baseline and monitoring period. For the baseline period we will generate a crude forest mask based on the COV thresholding method. This mask will be applied to study the monitoring period with cumluative sums of residuals agains a time series mean.  \n",
    "\n",
    "We explore the use of  Change Point Detection with *Cumulative Sum*. For details please see the EBD_SAR4 Notebook in the servor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('/notebooks/github/servir_training/notebooks_202006/Nadia_Exo')\n",
    "import ebdpy as ebd\n",
    "import glob\n",
    "\n",
    "from skimage import exposure # to enhance image display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gdal\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as patches  # Needed to draw rectangles\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case changes are made to the ebdpy module, reload it\n",
    "import importlib\n",
    "importlib.reload(ebd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the Project data set and time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT=True  # Change to False when one wants to only run the notebook to generate an output geotif.\n",
    "datapath='/Data/Sentinel-1/Peru/18MVT'\n",
    "\n",
    "userdatapath='/Userdata'\n",
    "os.makedirs(userdatapath,exist_ok=True)\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List the files in the Data directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob(os.path.join(datapath,\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_stacks=[x for x in files if x.endswith('_subset.tif') if not x.find('tsmetrics')>-1]\n",
    "ts_metrics=[x for x in files if x.endswith('_tsmetrics_subset.tif')]\n",
    "ts_stacks.sort()\n",
    "ts_metrics.sort()\n",
    "for i in zip(['ts_stacks','ts_metrics'],[ts_stacks,ts_metrics]):\n",
    "    print(i[0])\n",
    "    for j in i[1]:\n",
    "        print(i[1].index(j),j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's pick a time series stack index for a likepol and crosspol image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp=2\n",
    "lp=3\n",
    "likepol=ts_stacks[lp]\n",
    "crosspol=ts_stacks[xp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a polarization for the working data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_source=crosspol\n",
    "dsname=os.path.splitext(os.path.basename(ds_source))[0].replace('_mtfil','').replace('_subset','')\n",
    "dsname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the SAR Time Series data stack\n",
    "During reading, we are building a xarray dataset setting the time series index from the description attribute. For details take a look at the source code of `ebdpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ebd.read_sar(ds_source,dsname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data with coordinates xmin,ymin,xmax,ymax\n",
    "ul=[458152.776,9286360.706]\n",
    "\n",
    "lr=[]\n",
    "sizemeters=8000 #this is a buffer from the point (unit E?)Original was 15000\n",
    "if lr:\n",
    "    subset=[ul[0],ul[1],lr[0],lr[1]]\n",
    "else:\n",
    "    subset=[ul[0],ul[1],ul[0]+sizemeters,ul[1]-sizemeters]\n",
    "da=ebd.da_subset_xy(da,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_splitpoint='2019-01-14'\n",
    "da_baseline=da.sel(time=slice('2017-05-24',ts_splitpoint))\n",
    "da=da.sel(time=slice(ts_splitpoint,'2020-04-14')).chunk({'x':-1,'y':-1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a crude forest/non-forest mask for the pre period from COV thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=da_baseline.std('time')/da_baseline.mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image(cov,kdims=['x','y']).opts(cmap='viridis',width=600,height=600,tools=['hover']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_thres=0.14\n",
    "forest_mask=cov<cov_thres\n",
    "forest_mask.plot(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine a threshold from the plot above to make a mask and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_thres=0.14\n",
    "forest_mask=cov<cov_thres\n",
    "forest_mask.plot(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the mask (forest mask) to the data array ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.coords['mask'] = (('y', 'x'), forest_mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and apply it on the data array itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = da.where(da.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We could skip the step of adding the mask to the data array and supply forest_mask straight to the da.where() function.\n",
    "\n",
    "    da = da.where(forest_mask)\n",
    "\n",
    "Check with a plot if this worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.mean('time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Plot of Time Series Max/Min Difference from the Mean\n",
    "\n",
    "Let's make a plot of time series metrics where we take the difference with the mean of the maximum and minimum backscatter values. We generate three RGB bands for that from the time series stack Mean, Mean-Min, and Max-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=da.mean('time')\n",
    "r=g-da.min('time')\n",
    "b=da.max('time')-g\n",
    "rgb=np.dstack([r.values,g.values,b.values])\n",
    "# Let's histogram equalize the data\n",
    "rgb_stretched=ebd.rgb_stretch(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd.rgb_plot(rgb_stretched,da,f\"RGB Time Series Mean-Min/Mean/Max-Mean ({dsname})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding:\n",
    "\n",
    "Looks like the areas for degradation can be identified. Let's see if we can find a classifier for it. \n",
    "\n",
    "## Select the time series segment to analyze\n",
    "\n",
    "Let's see if we can find trends of change by comparing the minimum SAR backscatter in each quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with a subsetted resampled stack uncomment here\n",
    "# resampler = da.resample(time='36D',keep_attrs=True)\n",
    "# mymetric='median'\n",
    "# qmetric=resampler.median(keep_attrs=True).chunk({'x':-1,'y':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the entire stack\n",
    "mymetric='all'\n",
    "qmetric=da.chunk({'x':-1,'y':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmetric.plot(col='time',col_wrap=5,cmap='viridis',vmin=0.01,vmax=0.09,xticks=[],yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(12,8))\n",
    "qmetric.mean(dim=['x','y']).plot(ax=ax,label='mean')\n",
    "qmetric.min(dim=['x','y']).plot(ax=ax,label='min')\n",
    "#qmetric.max(dim=['x','y']).plot(ax=ax,label='max')\n",
    "q=[0.02,.98]\n",
    "pl,ph=qmetric.quantile(q,dim=['x','y'])\n",
    "pl.plot(ax=ax,label=f'p{q[0]*100:.0f}')\n",
    "ph.plot(ax=ax,label=f'p{q[1]*100:.0f}')\n",
    "ax.set_title(f'Time series means of quarterly metric \"{mymetric}\"')\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Residuals of the resampled metric to the mean of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmetric.mean('time').plot(vmin=0.01,vmax=0.1,figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmetric_residuals=qmetric-qmetric.mean('time').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmetric_residuals.plot(col='time',col_wrap=4,figsize=(16,20),vmin=-0.02,vmax=0.02,cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = qmetric_residuals.cumsum('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=[459660.561,9285273.414]\n",
    "pt=[459321.414,9283179.227]\n",
    "\n",
    "Spt= S.sel({'x':pt[0],'y':pt[1]},method='nearest').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(12,6))\n",
    "Spt.plot(ax=ax)\n",
    "ax.set_title(f'S Curve at Point {pt}')\n",
    "ax.set_ylabel('Cumulative Sum of Residual from Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S.plot(col='time',col_wrap=4,figsize=(16,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smax=S.where(S.mask).max('time')\n",
    "Smin=S.where(S.mask).min('time')\n",
    "SDiff=Smax-abs(Smin)\n",
    "Smax.plot(figsize=(12,12),cmap='viridis')\n",
    "Smin.plot(figsize=(12,12),cmap='viridis')\n",
    "SDiff.plot(figsize=(12,12),cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image(SDiff,kdims=['x','y']).opts(width=700,height=700,cmap='viridis',tools=['hover'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a threshold for SDiff as classifier\n",
    "\n",
    "We can pick a threshold from the interactive plot. Or:\n",
    "\n",
    "Because we applied a forest mask to our subsetted data stack, we can determine a threshold for disturbance as a multiple of the standard deviation of the mean as we assume Gaussian Distribution in the SDiff pdf:\n",
    "\n",
    "$thres = \\overline{S_{Diff}} + x \\times \\sigma_{S_{Diff}}$\n",
    "\n",
    "with \n",
    "$x$  factor for $\\sigma$ multiplication from the mean.\n",
    "\n",
    "Let's plot the histogram to confirm a quasi Gaussian pdf for SDiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(SDiff.values.flatten(),bins=200)\n",
    "_=plt.title('Histogram of SDiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2.5\n",
    "dthres=float(SDiff.mean()+x*SDiff.std())\n",
    "ndthres=float(SDiff.mean()-x*SDiff.std())\n",
    "# print(f'Setting the threshold for SDiff to {x}*Std.Dev.: {dthres:.2f}')\n",
    "print(f'Setting the threshold for SDiff to {x}*Std.Dev.: {ndthres:.2f} {dthres:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the threshold to the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disturbance=SDiff>dthres\n",
    "disturbance=np.logical_or(SDiff>dthres, SDiff<ndthres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "disturbance.plot(cmap='Reds',figsize=(10,8)) #change 'Reds' for 'Gray'\n",
    "_=plt.title(f'Forest Disturbance after {ts_splitpoint} from CumSum Classifier. SDiff Threshold: {dthres:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to perform\n",
    "\n",
    "We are skipping for this workshop the bootstrapping validation steps for the validation of the points. \n",
    "See the Notebook EBD_SAR4 from the 2019 Training in Colombia and Peru. Also in that Notebook are the steps for finding the change date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the result to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=os.path.join(userdatapath,f'{dsname}_CumSum_disturbance_Loreto-Peru_{dthres*10000:.0f}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeoT=ebd.transform2geotras(da.transform)\n",
    "GeoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tif\n",
    "ebd.CreateGeoTiff(Array=disturbance.values,Name=Name,GeoT=GeoT,ref_image=ds_source,overwrite=True)\n",
    "print(gdal.Info(Name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "- Change test sites\n",
    "- Look at the effect of using cross-polarized versus like-polarized polarizations \n",
    "- find other time series subsets e.g. build a mask for 2017 and process only for 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ebd]",
   "language": "python",
   "name": "conda-env-ebd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
